{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef8b87e",
   "metadata": {},
   "source": [
    "# Model Testing Framework - Upload Model & Dataset\n",
    "This notebook allows you to upload a pre-trained machine learning model and a test dataset to evaluate the model's performance using Recall and F1-score metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a4592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install scikit-learn pandas numpy matplotlib seaborn imbalanced-learn scipy openpyxl plotly\n",
    "\n",
    "# Import all necessary libraries   \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    f1_score, recall_score, precision_score, accuracy_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import arff\n",
    "from io import StringIO\n",
    "\n",
    "# Google Colab file upload\n",
    "from google.colab import files\n",
    "\n",
    "print(\"‚úÖ All packages installed and imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2557d53",
   "metadata": {},
   "source": [
    "## 1. Upload Your Pre-trained Model\n",
    "Upload your saved machine learning model (.pkl file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367931ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÅ Upload your pre-trained model (.pkl file)\")\n",
    "print(\"Supported model types: KNN, SVM, Random Forest, Logistic Regression, etc.\")\n",
    "\n",
    "uploaded_model = files.upload()\n",
    "model_filename = list(uploaded_model.keys())[0]\n",
    "\n",
    "try:\n",
    "    # Load the model\n",
    "    with open(model_filename, 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully: {model_filename}\")\n",
    "    \n",
    "    # Check model structure and extract components\n",
    "    if isinstance(model_data, dict):\n",
    "        print(\"\\nüìä Model package contents:\")\n",
    "        for key in model_data.keys():\n",
    "            print(f\"  - {key}: {type(model_data[key])}\")\n",
    "        \n",
    "        # Extract model components based on common structures\n",
    "        if 'model' in model_data:\n",
    "            model = model_data['model']\n",
    "            print(f\"\\nü§ñ Model type: {type(model).__name__}\")\n",
    "        else:\n",
    "            # If the pickle file contains the model directly\n",
    "            model = model_data\n",
    "            print(f\"\\nü§ñ Model type: {type(model).__name__}\")\n",
    "        \n",
    "        # Extract other components if available\n",
    "        scaler = model_data.get('scaler', None)\n",
    "        selected_features = model_data.get('selected_features', None)\n",
    "        selected_feature_indices = model_data.get('selected_feature_indices', None)\n",
    "        feature_names = model_data.get('feature_names', None)\n",
    "        hyperparameters = model_data.get('hyperparameters', model_data.get('best_hyperparameters', None))\n",
    "        \n",
    "        print(f\"\\nüìã Additional components:\")\n",
    "        print(f\"  - Scaler: {'‚úÖ' if scaler is not None else '‚ùå'}\")\n",
    "        print(f\"  - Selected features: {'‚úÖ' if selected_features is not None else '‚ùå'}\")\n",
    "        print(f\"  - Feature names: {'‚úÖ' if feature_names is not None else '‚ùå'}\")\n",
    "        print(f\"  - Hyperparameters: {'‚úÖ' if hyperparameters is not None else '‚ùå'}\")\n",
    "        \n",
    "        if hyperparameters:\n",
    "            print(f\"\\n‚öôÔ∏è Model hyperparameters:\")\n",
    "            for param, value in hyperparameters.items():\n",
    "                print(f\"  - {param}: {value}\")\n",
    "        \n",
    "    else:\n",
    "        # Direct model object\n",
    "        model = model_data\n",
    "        scaler = None\n",
    "        selected_features = None\n",
    "        selected_feature_indices = None\n",
    "        feature_names = None\n",
    "        hyperparameters = None\n",
    "        print(f\"\\nü§ñ Direct model type: {type(model).__name__}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {str(e)}\")\n",
    "    print(\"Please ensure you've uploaded a valid pickle (.pkl) file containing a trained model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13473eb",
   "metadata": {},
   "source": [
    "## 2. Upload Your Test Dataset\n",
    "Upload your test dataset (supports .csv, .arff, .xlsx formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÅ Upload your test dataset\")\n",
    "print(\"Supported formats: .csv, .arff, .xlsx\")\n",
    "\n",
    "uploaded_data = files.upload()\n",
    "data_filename = list(uploaded_data.keys())[0]\n",
    "file_extension = data_filename.split('.')[-1].lower()\n",
    "\n",
    "try:\n",
    "    # Load dataset based on file type\n",
    "    if file_extension == 'csv':\n",
    "        df = pd.read_csv(data_filename)\n",
    "        print(f\"‚úÖ CSV file loaded: {data_filename}\")\n",
    "    \n",
    "    elif file_extension == 'arff':\n",
    "        with open(data_filename, 'r') as f:\n",
    "            content = f.read()\n",
    "        data, meta = arff.loadarff(StringIO(content))\n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"‚úÖ ARFF file loaded: {data_filename}\")\n",
    "    \n",
    "    elif file_extension in ['xlsx', 'xls']:\n",
    "        df = pd.read_excel(data_filename)\n",
    "        print(f\"‚úÖ Excel file loaded: {data_filename}\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
    "    \n",
    "    print(f\"\\nüìä Dataset shape: {df.shape}\")\n",
    "    print(f\"üìä Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nüìã First 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum().sum()\n",
    "    print(f\"\\nüîç Missing values: {missing_values}\")\n",
    "    \n",
    "    if missing_values > 0:\n",
    "        print(\"‚ö†Ô∏è Missing values detected per column:\")\n",
    "        missing_per_col = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "        for col, count in missing_per_col.items():\n",
    "            print(f\"  - {col}: {count} missing values\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {str(e)}\")\n",
    "    print(\"Please ensure you've uploaded a valid dataset file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a779b",
   "metadata": {},
   "source": [
    "## 3. Configure Target Column and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c255848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically detect target column or let user specify\n",
    "print(\"üéØ Configure target column and preprocessing\")\n",
    "print(\"\\nAvailable columns:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    unique_vals = df[col].nunique()\n",
    "    data_type = df[col].dtype\n",
    "    print(f\"  {i}: {col} (dtype: {data_type}, unique values: {unique_vals})\")\n",
    "\n",
    "# Common target column names\n",
    "common_targets = ['bug', 'defect', 'class', 'target', 'label', 'y', 'outcome', 'diagnosis']\n",
    "detected_target = None\n",
    "\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if col_lower in common_targets or col_lower.endswith('_class') or col_lower.endswith('_label'):\n",
    "        # Check if it's binary or has few unique values (classification)\n",
    "        if df[col].nunique() <= 10:\n",
    "            detected_target = col\n",
    "            break\n",
    "\n",
    "if detected_target:\n",
    "    print(f\"\\nüéØ Auto-detected target column: '{detected_target}'\")\n",
    "    target_col = detected_target\n",
    "else:\n",
    "    print(\"\\n‚ùì Please specify the target column name:\")\n",
    "    target_col = input(\"Enter target column name: \").strip()\n",
    "\n",
    "if target_col not in df.columns:\n",
    "    print(f\"‚ùå Column '{target_col}' not found in dataset!\")\n",
    "    print(\"Available columns:\", list(df.columns))\n",
    "else:\n",
    "    print(f\"‚úÖ Using target column: '{target_col}'\")\n",
    "    \n",
    "    # Analyze target column\n",
    "    print(f\"\\nüìä Target column analysis:\")\n",
    "    print(f\"  - Data type: {df[target_col].dtype}\")\n",
    "    print(f\"  - Unique values: {df[target_col].nunique()}\")\n",
    "    print(f\"  - Value counts:\")\n",
    "    print(df[target_col].value_counts())\n",
    "    \n",
    "    # Convert target to binary if needed\n",
    "    if df[target_col].dtype == object or df[target_col].dtype.name == 'bytes':\n",
    "        # Handle byte strings (common in ARFF files)\n",
    "        if df[target_col].dtype.name == 'bytes':\n",
    "            df[target_col] = df[target_col].apply(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
    "        \n",
    "        # Convert to binary\n",
    "        unique_values = df[target_col].unique()\n",
    "        print(f\"\\nüîÑ Converting categorical target to binary:\")\n",
    "        print(f\"  - Unique values: {unique_values}\")\n",
    "        \n",
    "        # Auto-detect positive class\n",
    "        positive_indicators = ['Y', 'yes', 'true', 'True', '1', 'positive', 'defect', 'bug', 'M']\n",
    "        positive_class = None\n",
    "        \n",
    "        for val in unique_values:\n",
    "            if str(val) in positive_indicators:\n",
    "                positive_class = val\n",
    "                break\n",
    "        \n",
    "        if positive_class is None:\n",
    "            positive_class = unique_values[1] if len(unique_values) > 1 else unique_values[0]\n",
    "        \n",
    "        print(f\"  - Positive class: '{positive_class}' -> 1\")\n",
    "        print(f\"  - Negative class: others -> 0\")\n",
    "        \n",
    "        y = df[target_col].apply(lambda x: 1 if x == positive_class else 0)\n",
    "    else:\n",
    "        y = df[target_col].copy()\n",
    "    \n",
    "    # Extract features\n",
    "    X = df.drop(columns=[target_col])\n",
    "    \n",
    "    print(f\"\\nüìä Preprocessed data:\")\n",
    "    print(f\"  - Features shape: {X.shape}\")\n",
    "    print(f\"  - Target shape: {y.shape}\")\n",
    "    print(f\"  - Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3079b5",
   "metadata": {},
   "source": [
    "## 4. Apply Model-Specific Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df109c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Applying model-specific preprocessing...\")\n",
    "\n",
    "# Store original feature names\n",
    "original_feature_names = X.columns.tolist()\n",
    "X_processed = X.copy()\n",
    "\n",
    "# Handle non-numeric columns\n",
    "non_numeric_cols = X_processed.select_dtypes(include=['object']).columns\n",
    "if len(non_numeric_cols) > 0:\n",
    "    print(f\"\\nüîÑ Converting non-numeric columns: {list(non_numeric_cols)}\")\n",
    "    \n",
    "    for col in non_numeric_cols:\n",
    "        if X_processed[col].dtype.name == 'bytes':\n",
    "            # Handle byte strings\n",
    "            X_processed[col] = X_processed[col].apply(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
    "        \n",
    "        # Simple label encoding\n",
    "        le = LabelEncoder()\n",
    "        X_processed[col] = le.fit_transform(X_processed[col].astype(str))\n",
    "\n",
    "# Handle missing values\n",
    "if X_processed.isnull().sum().sum() > 0:\n",
    "    print(\"\\nüîß Handling missing values (filling with median)...\")\n",
    "    X_processed = X_processed.fillna(X_processed.median())\n",
    "\n",
    "# Apply scaling if model has a scaler\n",
    "if scaler is not None:\n",
    "    print(\"\\nüìè Applying model's scaler...\")\n",
    "    X_scaled = scaler.transform(X_processed)\n",
    "    X_processed = pd.DataFrame(X_scaled, columns=X_processed.columns)\n",
    "    print(\"‚úÖ Data scaled successfully\")\n",
    "else:\n",
    "    print(\"\\nüìè No scaler found in model. Using original feature values.\")\n",
    "\n",
    "# Apply feature selection if model has selected features\n",
    "if selected_features is not None and len(selected_features) > 0:\n",
    "    print(f\"\\nüéØ Applying feature selection ({len(selected_features)} features)...\")\n",
    "    print(f\"Selected features: {selected_features}\")\n",
    "    \n",
    "    # Check if selected features exist in current dataset\n",
    "    available_features = [f for f in selected_features if f in X_processed.columns]\n",
    "    missing_features = [f for f in selected_features if f not in X_processed.columns]\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"‚ö†Ô∏è Warning: Some selected features are missing: {missing_features}\")\n",
    "    \n",
    "    if available_features:\n",
    "        X_processed = X_processed[available_features]\n",
    "        print(f\"‚úÖ Using {len(available_features)} selected features\")\n",
    "    else:\n",
    "        print(\"‚ùå No selected features found in dataset!\")\n",
    "\n",
    "elif selected_feature_indices is not None:\n",
    "    print(f\"\\nüéØ Applying feature selection by indices ({len(selected_feature_indices)} features)...\")\n",
    "    if max(selected_feature_indices) < X_processed.shape[1]:\n",
    "        X_processed = X_processed.iloc[:, selected_feature_indices]\n",
    "        print(f\"‚úÖ Using {len(selected_feature_indices)} selected features\")\n",
    "    else:\n",
    "        print(\"‚ùå Feature indices exceed dataset dimensions!\")\n",
    "\n",
    "print(f\"\\nüìä Final processed data shape: {X_processed.shape}\")\n",
    "print(f\"üìä Feature columns: {list(X_processed.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef09d5e",
   "metadata": {},
   "source": [
    "## 5. Make Predictions and Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4493b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÆ Making predictions...\")\n",
    "\n",
    "try:\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_processed)\n",
    "    \n",
    "    # Get prediction probabilities if available\n",
    "    try:\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_processed)[:, 1]  # Probability of positive class\n",
    "        elif hasattr(model, 'decision_function'):\n",
    "            y_pred_proba = model.decision_function(X_processed)\n",
    "        else:\n",
    "            y_pred_proba = None\n",
    "    except:\n",
    "        y_pred_proba = None\n",
    "    \n",
    "    print(f\"‚úÖ Predictions completed!\")\n",
    "    print(f\"üìä Predictions shape: {y_pred.shape}\")\n",
    "    print(f\"üìä Prediction distribution: {pd.Series(y_pred).value_counts().to_dict()}\")\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    print(\"\\nüìä PERFORMANCE METRICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Main metrics\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    \n",
    "    print(f\"üéØ F1-Score:  {f1:.4f}\")\n",
    "    print(f\"üéØ Recall:    {recall:.4f}\")\n",
    "    print(f\"üéØ Precision: {precision:.4f}\")\n",
    "    print(f\"üéØ Accuracy:  {accuracy:.4f}\")\n",
    "    \n",
    "    # AUC if probabilities available\n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            auc = roc_auc_score(y, y_pred_proba)\n",
    "            print(f\"üéØ AUC-ROC:   {auc:.4f}\")\n",
    "        except:\n",
    "            auc = None\n",
    "            print(f\"üéØ AUC-ROC:   Not available\")\n",
    "    else:\n",
    "        auc = None\n",
    "        print(f\"üéØ AUC-ROC:   Not available (no probabilities)\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(\"\\nüìä CONFUSION MATRIX\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"True Negatives:  {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives:  {tp}\")\n",
    "    \n",
    "    # Additional derived metrics\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "    \n",
    "    print(\"\\nüìä ADDITIONAL METRICS\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Specificity (TNR): {specificity:.4f}\")\n",
    "    print(f\"Negative Pred. Value: {npv:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nüìä DETAILED CLASSIFICATION REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    print(classification_report(y, y_pred, target_names=['Class 0', 'Class 1']))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during prediction: {str(e)}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"- Check if the model expects the same number of features as your dataset\")\n",
    "    print(\"- Ensure feature names match (if feature selection was used)\")\n",
    "    print(\"- Verify data types and preprocessing requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25658370",
   "metadata": {},
   "source": [
    "## 6. Create Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'y_pred' in locals():\n",
    "    print(\"üìä Creating visualizations...\")\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Confusion Matrix Heatmap\n",
    "    plt.subplot(2, 3, 1)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 2. Metrics Bar Plot\n",
    "    plt.subplot(2, 3, 2)\n",
    "    metrics = ['F1-Score', 'Recall', 'Precision', 'Accuracy']\n",
    "    values = [f1, recall, precision, accuracy]\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "    \n",
    "    bars = plt.bar(metrics, values, color=colors, alpha=0.8)\n",
    "    plt.title('Performance Metrics', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1.1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 3. Prediction Distribution\n",
    "    plt.subplot(2, 3, 3)\n",
    "    pred_counts = pd.Series(y_pred).value_counts()\n",
    "    plt.pie(pred_counts.values, labels=[f'Class {i}' for i in pred_counts.index], \n",
    "            autopct='%1.1f%%', startangle=90, colors=['#FF9999', '#66B2FF'])\n",
    "    plt.title('Prediction Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 4. True vs Predicted scatter plot (if enough samples)\n",
    "    plt.subplot(2, 3, 4)\n",
    "    if len(y) <= 1000:  # Only for smaller datasets to avoid clutter\n",
    "        # Add some jitter for better visualization\n",
    "        y_jitter = y + np.random.normal(0, 0.05, len(y))\n",
    "        pred_jitter = y_pred + np.random.normal(0, 0.05, len(y_pred))\n",
    "        \n",
    "        plt.scatter(y_jitter, pred_jitter, alpha=0.6, c=y_pred, cmap='RdYlBu')\n",
    "        plt.xlabel('True Labels')\n",
    "        plt.ylabel('Predicted Labels')\n",
    "        plt.title('True vs Predicted Labels', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Add diagonal line for perfect predictions\n",
    "        plt.plot([0, 1], [0, 1], 'r--', alpha=0.8, linewidth=2)\n",
    "    else:\n",
    "        # For larger datasets, show a summary table\n",
    "        summary_data = {\n",
    "            'Metric': ['Total Samples', 'Correct Predictions', 'Incorrect Predictions', 'Accuracy %'],\n",
    "            'Value': [len(y), (y == y_pred).sum(), (y != y_pred).sum(), f'{accuracy*100:.1f}%']\n",
    "        }\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        table = plt.table(cellText=summary_df.values, colLabels=summary_df.columns,\n",
    "                         cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(12)\n",
    "        table.scale(1, 2)\n",
    "        plt.axis('off')\n",
    "        plt.title('Prediction Summary', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 5. ROC Curve (if probabilities available)\n",
    "    plt.subplot(2, 3, 5)\n",
    "    if y_pred_proba is not None and auc is not None:\n",
    "        fpr, tpr, _ = roc_curve(y, y_pred_proba)\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC curve (AUC = {auc:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'ROC Curve\\nNot Available\\n(No Probabilities)', \n",
    "                ha='center', va='center', fontsize=12, \n",
    "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 6. Precision-Recall Curve (if probabilities available)\n",
    "    plt.subplot(2, 3, 6)\n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            precision_curve, recall_curve, _ = precision_recall_curve(y, y_pred_proba)\n",
    "            plt.plot(recall_curve, precision_curve, color='blue', lw=2)\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "        except:\n",
    "            plt.text(0.5, 0.5, 'Precision-Recall\\nCurve Error', \n",
    "                    ha='center', va='center', fontsize=12,\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))\n",
    "            plt.xlim(0, 1)\n",
    "            plt.ylim(0, 1)\n",
    "            plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Precision-Recall\\nCurve Not Available\\n(No Probabilities)', \n",
    "                ha='center', va='center', fontsize=12,\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualizations created successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå No predictions available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53c45ba",
   "metadata": {},
   "source": [
    "## 7. Detailed Analysis and Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e64ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'y_pred' in locals():\n",
    "    print(\"üìã DETAILED ANALYSIS REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Model Information\n",
    "    print(\"\\nü§ñ MODEL INFORMATION:\")\n",
    "    print(f\"  ‚Ä¢ Model Type: {type(model).__name__}\")\n",
    "    print(f\"  ‚Ä¢ Model File: {model_filename}\")\n",
    "    if hyperparameters:\n",
    "        print(f\"  ‚Ä¢ Hyperparameters:\")\n",
    "        for param, value in hyperparameters.items():\n",
    "            print(f\"    - {param}: {value}\")\n",
    "    \n",
    "    # Dataset Information\n",
    "    print(\"\\nüìä DATASET INFORMATION:\")\n",
    "    print(f\"  ‚Ä¢ Dataset File: {data_filename}\")\n",
    "    print(f\"  ‚Ä¢ Total Samples: {len(y)}\")\n",
    "    print(f\"  ‚Ä¢ Total Features: {X_processed.shape[1]}\")\n",
    "    print(f\"  ‚Ä¢ Target Column: {target_col}\")\n",
    "    print(f\"  ‚Ä¢ Class Distribution: {dict(pd.Series(y).value_counts())}\")\n",
    "    \n",
    "    # Performance Summary\n",
    "    print(\"\\nüéØ PERFORMANCE SUMMARY:\")\n",
    "    print(f\"  ‚Ä¢ F1-Score: {f1:.4f} {'üü¢' if f1 >= 0.8 else 'üü°' if f1 >= 0.6 else 'üî¥'}\")\n",
    "    print(f\"  ‚Ä¢ Recall: {recall:.4f} {'üü¢' if recall >= 0.8 else 'üü°' if recall >= 0.6 else 'üî¥'}\")\n",
    "    print(f\"  ‚Ä¢ Precision: {precision:.4f} {'üü¢' if precision >= 0.8 else 'üü°' if precision >= 0.6 else 'üî¥'}\")\n",
    "    print(f\"  ‚Ä¢ Accuracy: {accuracy:.4f} {'üü¢' if accuracy >= 0.8 else 'üü°' if accuracy >= 0.6 else 'üî¥'}\")\n",
    "    if auc is not None:\n",
    "        print(f\"  ‚Ä¢ AUC-ROC: {auc:.4f} {'üü¢' if auc >= 0.8 else 'üü°' if auc >= 0.6 else 'üî¥'}\")\n",
    "    \n",
    "    # Classification Performance per Class\n",
    "    print(\"\\nüìä PER-CLASS PERFORMANCE:\")\n",
    "    for class_label in [0, 1]:\n",
    "        class_mask = (y == class_label)\n",
    "        class_correct = ((y == class_label) & (y_pred == class_label)).sum()\n",
    "        class_total = class_mask.sum()\n",
    "        class_accuracy = class_correct / class_total if class_total > 0 else 0\n",
    "        print(f\"  ‚Ä¢ Class {class_label}: {class_correct}/{class_total} correct ({class_accuracy:.3f})\")\n",
    "    \n",
    "    # Error Analysis\n",
    "    print(\"\\n‚ùå ERROR ANALYSIS:\")\n",
    "    false_positives = ((y == 0) & (y_pred == 1)).sum()\n",
    "    false_negatives = ((y == 1) & (y_pred == 0)).sum()\n",
    "    print(f\"  ‚Ä¢ False Positives: {false_positives} ({false_positives/len(y)*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ False Negatives: {false_negatives} ({false_negatives/len(y)*100:.1f}%)\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "    if f1 < 0.6:\n",
    "        print(\"  ‚ö†Ô∏è Low F1-score suggests model needs improvement\")\n",
    "        print(\"     Consider: hyperparameter tuning, feature engineering, or different algorithm\")\n",
    "    \n",
    "    if recall < 0.7 and precision > 0.8:\n",
    "        print(\"  ‚ö†Ô∏è High precision but low recall - model is conservative\")\n",
    "        print(\"     Consider: adjusting decision threshold or addressing class imbalance\")\n",
    "    \n",
    "    if precision < 0.7 and recall > 0.8:\n",
    "        print(\"  ‚ö†Ô∏è High recall but low precision - model is aggressive\")\n",
    "        print(\"     Consider: more restrictive decision criteria or better feature selection\")\n",
    "    \n",
    "    if accuracy > 0.8 and f1 > 0.8 and recall > 0.8:\n",
    "        print(\"  ‚úÖ Excellent performance across all metrics!\")\n",
    "    elif accuracy > 0.7 and f1 > 0.7:\n",
    "        print(\"  ‚úÖ Good performance - model is working well\")\n",
    "    \n",
    "    # Feature Information\n",
    "    if selected_features:\n",
    "        print(\"\\nüéØ FEATURE SELECTION APPLIED:\")\n",
    "        print(f\"  ‚Ä¢ Original Features: {len(original_feature_names)}\")\n",
    "        print(f\"  ‚Ä¢ Selected Features: {len(selected_features)}\")\n",
    "        print(f\"  ‚Ä¢ Reduction: {(1 - len(selected_features)/len(original_feature_names))*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No predictions available for detailed analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd124d0",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b111f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'y_pred' in locals():\n",
    "    print(\"üíæ Exporting results to Excel...\")\n",
    "    \n",
    "    # Create timestamp for filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_filename = f'model_evaluation_results_{timestamp}.xlsx'\n",
    "    \n",
    "    try:\n",
    "        with pd.ExcelWriter(results_filename, engine='openpyxl') as writer:\n",
    "            \n",
    "            # Sheet 1: Summary Metrics\n",
    "            summary_metrics = pd.DataFrame({\n",
    "                'Metric': ['F1-Score', 'Recall', 'Precision', 'Accuracy', 'AUC-ROC', \n",
    "                          'True Positives', 'True Negatives', 'False Positives', 'False Negatives',\n",
    "                          'Specificity', 'NPV'],\n",
    "                'Value': [f1, recall, precision, accuracy, auc if auc else 'N/A',\n",
    "                         tp, tn, fp, fn, specificity, npv]\n",
    "            })\n",
    "            summary_metrics.to_excel(writer, sheet_name='Summary_Metrics', index=False)\n",
    "            \n",
    "            # Sheet 2: Detailed Results (predictions for each sample)\n",
    "            detailed_results = pd.DataFrame({\n",
    "                'Sample_Index': range(len(y)),\n",
    "                'True_Label': y.values if hasattr(y, 'values') else y,\n",
    "                'Predicted_Label': y_pred,\n",
    "                'Correct_Prediction': (y.values if hasattr(y, 'values') else y) == y_pred\n",
    "            })\n",
    "            \n",
    "            if y_pred_proba is not None:\n",
    "                detailed_results['Prediction_Probability'] = y_pred_proba\n",
    "            \n",
    "            detailed_results.to_excel(writer, sheet_name='Detailed_Predictions', index=False)\n",
    "            \n",
    "            # Sheet 3: Model Information\n",
    "            model_info_data = {\n",
    "                'Property': ['Model_Type', 'Model_File', 'Dataset_File', 'Target_Column',\n",
    "                            'Total_Samples', 'Total_Features', 'Selected_Features_Count',\n",
    "                            'Analysis_Timestamp'],\n",
    "                'Value': [type(model).__name__, model_filename, data_filename, target_col,\n",
    "                         len(y), X_processed.shape[1], \n",
    "                         len(selected_features) if selected_features else 'All',\n",
    "                         datetime.now().strftime('%Y-%m-%d %H:%M:%S')]\n",
    "            }\n",
    "            \n",
    "            if hyperparameters:\n",
    "                for param, value in hyperparameters.items():\n",
    "                    model_info_data['Property'].append(f'Hyperparameter_{param}')\n",
    "                    model_info_data['Value'].append(value)\n",
    "            \n",
    "            model_info_df = pd.DataFrame(model_info_data)\n",
    "            model_info_df.to_excel(writer, sheet_name='Model_Information', index=False)\n",
    "            \n",
    "            # Sheet 4: Confusion Matrix\n",
    "            cm_df = pd.DataFrame(cm, \n",
    "                               columns=['Predicted_0', 'Predicted_1'],\n",
    "                               index=['Actual_0', 'Actual_1'])\n",
    "            cm_df.to_excel(writer, sheet_name='Confusion_Matrix')\n",
    "            \n",
    "            # Sheet 5: Feature Information (if available)\n",
    "            if selected_features:\n",
    "                feature_info = pd.DataFrame({\n",
    "                    'Selected_Features': selected_features,\n",
    "                    'Feature_Index': range(len(selected_features))\n",
    "                })\n",
    "                feature_info.to_excel(writer, sheet_name='Selected_Features', index=False)\n",
    "        \n",
    "        print(f\"‚úÖ Results exported to: {results_filename}\")\n",
    "        \n",
    "        # Download the file\n",
    "        files.download(results_filename)\n",
    "        print(f\"üì• File downloaded successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting results: {str(e)}\")\n",
    "        \n",
    "        # Create a simple CSV as fallback\n",
    "        try:\n",
    "            fallback_filename = f'model_evaluation_summary_{timestamp}.csv'\n",
    "            summary_metrics.to_csv(fallback_filename, index=False)\n",
    "            print(f\"‚úÖ Fallback summary exported to: {fallback_filename}\")\n",
    "            files.download(fallback_filename)\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Error creating fallback file: {str(e2)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No results available for export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd1cbe",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Next Steps\n",
    "\n",
    "üéâ **Analysis Complete!**\n",
    "\n",
    "This notebook has successfully:\n",
    "- ‚úÖ Loaded your pre-trained model\n",
    "- ‚úÖ Processed your test dataset\n",
    "- ‚úÖ Applied model-specific preprocessing\n",
    "- ‚úÖ Generated predictions\n",
    "- ‚úÖ Calculated comprehensive performance metrics\n",
    "- ‚úÖ Created detailed visualizations\n",
    "- ‚úÖ Exported results to Excel\n",
    "\n",
    "### Key Metrics Summary:\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **Recall**: Ability to find all positive instances\n",
    "- **Precision**: Accuracy of positive predictions\n",
    "- **Accuracy**: Overall correctness of predictions\n",
    "\n",
    "### Next Steps:\n",
    "1. **Review the detailed metrics** to understand model performance\n",
    "2. **Analyze the confusion matrix** to identify error patterns\n",
    "3. **Consider model improvements** based on the recommendations\n",
    "4. **Test with additional datasets** to validate robustness\n",
    "5. **Fine-tune hyperparameters** if performance needs improvement\n",
    "\n",
    "### Files Generated:\n",
    "- **Excel Report**: Comprehensive results with multiple sheets\n",
    "- **Visualizations**: Performance charts and analysis plots\n",
    "\n",
    "Thank you for using this model evaluation framework! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
