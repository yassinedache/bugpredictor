{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-objective evaluation function\n",
    "def evaluate_knn_multi_cv(individual, X, y, cv_folds=5):\n",
    "    \"\"\"Evaluate KNN performance using cross-validation for multiple metrics (precision, recall, f1)\"\"\"\n",
    "    try:\n",
    "        # For feature selection only\n",
    "        feature_mask = individual\n",
    "        k = 5  # Default k\n",
    "        weights = 'uniform'  # Default weights\n",
    "        p = 2  # Default p (Euclidean)\n",
    "        \n",
    "        # Select features\n",
    "        selected_features = [i for i, mask in enumerate(feature_mask) if mask > 0.5]\n",
    "        if len(selected_features) == 0:\n",
    "            return (0.0, 0.0, 0.0)  # No features selected\n",
    "        \n",
    "        X_selected = X.iloc[:, selected_features]\n",
    "        \n",
    "        # Ensure k is valid (adjust max k based on dataset size)\n",
    "        max_k = min(20, len(X_selected) // 5)  # Use at most 1/5 of data size for k\n",
    "        k = max(1, min(k, max_k))\n",
    "        \n",
    "        # Create model\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights=weights, p=p)\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Calculate all three metrics\n",
    "        precision_scores = cross_val_score(knn, X_selected, y, cv=cv, scoring='precision')\n",
    "        recall_scores = cross_val_score(knn, X_selected, y, cv=cv, scoring='recall')\n",
    "        f1_scores = cross_val_score(knn, X_selected, y, cv=cv, scoring='f1')\n",
    "        \n",
    "        return (np.mean(precision_scores), np.mean(recall_scores), np.mean(f1_scores))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation error: {str(e)}\")\n",
    "        return (0.0, 0.0, 0.0)\n",
    "\n",
    "# Multi-objective feature selection workflow\n",
    "def run_ga_workflow_multi(X, y, n_runs=10, generations=20, pop_size=50):\n",
    "    \"\"\"Multi-objective feature selection optimization\"\"\"\n",
    "    print(f\"Running Multi-Objective Feature Selection (precision, recall, f1)\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        print(f\"  Run {run + 1}/{n_runs}\")\n",
    "        \n",
    "        # Setup GA\n",
    "        toolbox = base.Toolbox()\n",
    "        toolbox.register(\"attr_bool\", random.random)\n",
    "        toolbox.register(\"individual\", tools.initRepeat, creator.MultiIndividual, \n",
    "                        toolbox.attr_bool, n=X.shape[1])\n",
    "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "        toolbox.register(\"evaluate\", evaluate_knn_multi_cv, X=X, y=y)\n",
    "        toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "        toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n",
    "        toolbox.register(\"select\", tools.selNSGA2)\n",
    "        \n",
    "        # Run GA\n",
    "        population = toolbox.population(n=pop_size)\n",
    "        hof = tools.ParetoFront()\n",
    "        \n",
    "        # Use NSGA-II algorithm\n",
    "        algorithms.eaMuPlusLambda(population, toolbox, mu=pop_size, lambda_=pop_size,\n",
    "                               cxpb=0.7, mutpb=0.2, ngen=generations, halloffame=hof, verbose=False)\n",
    "        \n",
    "        # Store all non-dominated solutions\n",
    "        for ind in hof:\n",
    "            results.append(ind)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to test multi-objective model\n",
    "def test_multi_model(individual, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Test the multi-objective model on test set and return all metrics\"\"\"\n",
    "    try:\n",
    "        # Parse individual for feature selection\n",
    "        feature_mask = individual\n",
    "        k = 5  # Default k\n",
    "        weights = 'uniform'  # Default weights\n",
    "        p = 2  # Default p (Euclidean)\n",
    "        \n",
    "        # Select features\n",
    "        selected_features = [i for i, mask in enumerate(feature_mask) if mask > 0.5]\n",
    "        if len(selected_features) == 0:\n",
    "            return {metric: 0.0 for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc']}\n",
    "        \n",
    "        X_train_selected = X_train.iloc[:, selected_features]\n",
    "        X_test_selected = X_test.iloc[:, selected_features]\n",
    "        \n",
    "        # Train and test model\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights=weights, p=p)\n",
    "        knn.fit(X_train_selected, y_train)\n",
    "        y_pred = knn.predict(X_test_selected)\n",
    "        y_pred_proba = knn.predict_proba(X_test_selected)[:, 1]\n",
    "        \n",
    "        # Calculate all metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'pr_auc': average_precision_score(y_test, y_pred_proba),\n",
    "            'selected_features': len(selected_features)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Test error: {str(e)}\")\n",
    "        return {metric: 0.0 for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc', 'selected_features']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c8b0c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# KNN Genetic Algorithm Optimization for Promise Datasets\n",
    "# Complete implementation with 4 GA workflows and hyperparameter display\n",
    "\n",
    "# Install required libraries\n",
    "!pip install pandas numpy scikit-learn imbalanced-learn deap shap openpyxl matplotlib seaborn scipy\n",
    "\n",
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, accuracy_score, \n",
    "    roc_auc_score, average_precision_score, classification_report\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from deap import base, creator, tools, algorithms\n",
    "from scipy.io import arff\n",
    "import random\n",
    "import warnings\n",
    "import io\n",
    "from io import StringIO\n",
    "from google.colab import files\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"All libraries installed and imported successfully!\")\n",
    "\n",
    "# Create DEAP fitness and individual classes\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Maximize fitness\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Data upload and preprocessing functions\n",
    "def upload_and_load_files():\n",
    "    \"\"\"Upload CSV or ARFF files and return dictionary of dataframes\"\"\"\n",
    "    print(\"Please upload your Promise dataset files (CSV or ARFF format)...\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    datasets = {}\n",
    "    for filename in uploaded.keys():\n",
    "        if filename.endswith('.csv'):\n",
    "            df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
    "            datasets[filename.replace('.csv', '')] = df\n",
    "            print(f\"Loaded CSV {filename}: {df.shape}\")\n",
    "        elif filename.endswith('.arff'):\n",
    "            # Load ARFF file - fix for bytes-like object error\n",
    "            try:\n",
    "                # First try with default loading (binary mode)\n",
    "                data, meta = arff.loadarff(io.BytesIO(uploaded[filename]))\n",
    "                df = pd.DataFrame(data)\n",
    "            except TypeError:\n",
    "                # If we get a TypeError about bytes-like object, decode to string first\n",
    "                arff_content = uploaded[filename].decode('utf-8')\n",
    "                data, meta = arff.loadarff(io.StringIO(arff_content))\n",
    "                df = pd.DataFrame(data)\n",
    "            \n",
    "            # Convert byte strings to regular strings for object columns\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'object':\n",
    "                    try:\n",
    "                        df[col] = df[col].str.decode('utf-8')\n",
    "                    except AttributeError:\n",
    "                        # If it's not bytes, leave as is\n",
    "                        pass\n",
    "            \n",
    "            datasets[filename.replace('.arff', '')] = df\n",
    "            print(f\"Loaded ARFF {filename}: {df.shape}\")\n",
    "            print(f\"Attributes: {list(meta.names())}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "def preprocess_dataset(df, target_column=None):\n",
    "    \"\"\"Clean, encode, and prepare dataset for ML\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Impute missing values\n",
    "    if len(numeric_columns) > 0:\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "        df[numeric_columns] = num_imputer.fit_transform(df[numeric_columns])\n",
    "    \n",
    "    if len(categorical_columns) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df[categorical_columns] = cat_imputer.fit_transform(df[categorical_columns])\n",
    "    \n",
    "    # Handle target column - try common names if not specified\n",
    "    if target_column is None:\n",
    "        # Common target column names in Promise datasets (bug is now first priority)\n",
    "        possible_targets = ['bug', 'defects', 'class', 'defective', 'Class']\n",
    "        target_column = None\n",
    "        \n",
    "        for col in possible_targets:\n",
    "            if col in df.columns:\n",
    "                target_column = col\n",
    "                break\n",
    "        \n",
    "        if target_column is None:\n",
    "            # Ask user to specify target column\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "            target_column = input(\"Please specify the target column name: \")\n",
    "    \n",
    "    if target_column in df.columns:\n",
    "        y = df[target_column]\n",
    "        X = df.drop(columns=[target_column])\n",
    "        print(f\"Using '{target_column}' as target column\")\n",
    "    else:\n",
    "        # Assume last column is target\n",
    "        y = df.iloc[:, -1]\n",
    "        X = df.iloc[:, :-1]\n",
    "        print(f\"Using last column '{df.columns[-1]}' as target column\")\n",
    "    \n",
    "    # Encode categorical features\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # Encode target if categorical\n",
    "    if y.dtype == 'object' or y.dtype == 'bool':\n",
    "        target_encoder = LabelEncoder()\n",
    "        y = target_encoder.fit_transform(y)\n",
    "        print(f\"Target classes: {list(target_encoder.classes_)}\")\n",
    "        print(f\"Target encoding: false=0, true=1\")\n",
    "    else:\n",
    "        target_encoder = None\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    \n",
    "    return X_scaled, y, scaler, label_encoders, target_encoder\n",
    "\n",
    "def apply_smote(X, y, random_state=42):\n",
    "    \"\"\"Apply SMOTE to balance the dataset\"\"\"\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    \n",
    "    print(f\"Original class distribution: {np.bincount(y)}\")\n",
    "    print(f\"Balanced class distribution: {np.bincount(y_balanced)}\")\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "# Hyperparameter decoding and display functions\n",
    "def decode_hyperparameters(individual, workflow_type, feature_names=None):\n",
    "    \"\"\"Decode individual to human-readable hyperparameters\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    if workflow_type == 'features_only':\n",
    "        feature_mask = individual\n",
    "        result['k'] = 5  # Default\n",
    "        result['weights'] = 'uniform'  # Default\n",
    "        result['distance_metric'] = 'euclidean'  # Default\n",
    "        if feature_names:\n",
    "            selected_features = [feature_names[i] for i, mask in enumerate(feature_mask) if mask > 0.5]\n",
    "            result['selected_features'] = selected_features\n",
    "            result['num_features'] = len(selected_features)\n",
    "    elif workflow_type == 'params_only':\n",
    "        k = individual[0]\n",
    "        weights = 'distance' if individual[1] > 0.5 else 'uniform'\n",
    "        p = 1 if individual[2] < 0.33 else (2 if individual[2] < 0.66 else 3)\n",
    "        \n",
    "        result['k'] = int(k * 20) if isinstance(k, float) else k\n",
    "        result['weights'] = weights\n",
    "        result['distance_metric'] = 'manhattan' if p == 1 else ('euclidean' if p == 2 else 'minkowski')\n",
    "        result['num_features'] = 'all'\n",
    "        if feature_names:\n",
    "            result['selected_features'] = feature_names\n",
    "    else:  # joint or sequential\n",
    "        feature_mask = individual[:-3]\n",
    "        k = individual[-3]\n",
    "        weights = 'distance' if individual[-2] > 0.5 else 'uniform'\n",
    "        p = 1 if individual[-1] < 0.33 else (2 if individual[-1] < 0.66 else 3)\n",
    "        \n",
    "        result['k'] = int(k * 20) if isinstance(k, float) else k\n",
    "        result['weights'] = weights\n",
    "        result['distance_metric'] = 'manhattan' if p == 1 else ('euclidean' if p == 2 else 'minkowski')\n",
    "        if feature_names:\n",
    "            selected_features = [feature_names[i] for i, mask in enumerate(feature_mask) if mask > 0.5]\n",
    "            result['selected_features'] = selected_features\n",
    "            result['num_features'] = len(selected_features)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def display_best_hyperparameters(best_individuals, workflow_names, metrics, feature_names=None):\n",
    "    \"\"\"Display best hyperparameters for each workflow and metric combination\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST HYPERPARAMETERS BY WORKFLOW AND OPTIMIZATION METRIC\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"\\nüéØ OPTIMIZATION METRIC: {metric.upper()}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for workflow_name in workflow_names:\n",
    "            workflow_type = workflow_name.split('_', 1)[1]\n",
    "            \n",
    "            if (metric, workflow_name) in best_individuals:\n",
    "                individuals = best_individuals[(metric, workflow_name)]\n",
    "                \n",
    "                print(f\"\\nüìä Workflow: {workflow_name.replace('_', ' ').title()}\")\n",
    "                \n",
    "                # Decode all individuals\n",
    "                decoded_params = []\n",
    "                for individual in individuals:\n",
    "                    params = decode_hyperparameters(individual, workflow_type, feature_names)\n",
    "                    decoded_params.append(params)\n",
    "                \n",
    "                # Calculate statistics for hyperparameters\n",
    "                if decoded_params:\n",
    "                    # K values\n",
    "                    k_values = [p['k'] for p in decoded_params]\n",
    "                    k_mean = statistics.mean(k_values)\n",
    "                    k_mode = statistics.mode(k_values) if k_values else k_values[0]\n",
    "                    \n",
    "                    # Weights\n",
    "                    weights_values = [p['weights'] for p in decoded_params]\n",
    "                    weights_mode = statistics.mode(weights_values)\n",
    "                    \n",
    "                    # Distance metrics\n",
    "                    distance_values = [p['distance_metric'] for p in decoded_params]\n",
    "                    distance_mode = statistics.mode(distance_values)\n",
    "                    \n",
    "                    print(f\"  ‚Ä¢ K neighbors: {k_mode} (mode), {k_mean:.1f} (mean), range: {min(k_values)}-{max(k_values)}\")\n",
    "                    print(f\"  ‚Ä¢ Weight scheme: {weights_mode}\")\n",
    "                    print(f\"  ‚Ä¢ Distance metric: {distance_mode}\")\n",
    "                    \n",
    "                    # Feature selection statistics\n",
    "                    if workflow_type != 'params_only':\n",
    "                        num_features = [p['num_features'] for p in decoded_params]\n",
    "                        if num_features:\n",
    "                            feat_mean = statistics.mean(num_features)\n",
    "                            feat_mode = statistics.mode(num_features)\n",
    "                            print(f\"  ‚Ä¢ Features selected: {feat_mode} (mode), {feat_mean:.1f} (mean), range: {min(num_features)}-{max(num_features)}\")\n",
    "                            \n",
    "                            # Most frequently selected features\n",
    "                            if feature_names and workflow_type != 'params_only':\n",
    "                                feature_counts = defaultdict(int)\n",
    "                                for p in decoded_params:\n",
    "                                    if 'selected_features' in p:\n",
    "                                        for feat in p['selected_features']:\n",
    "                                            feature_counts[feat] += 1\n",
    "                                \n",
    "                                if feature_counts:\n",
    "                                    top_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "                                    print(f\"  ‚Ä¢ Top selected features:\")\n",
    "                                    for feat, count in top_features:\n",
    "                                        percentage = (count / len(decoded_params)) * 100\n",
    "                                        print(f\"    - {feat}: {count}/{len(decoded_params)} runs ({percentage:.1f}%)\")\n",
    "                    else:\n",
    "                        print(f\"  ‚Ä¢ Features: All features used\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Evaluation functions\n",
    "def evaluate_knn_cv(individual, X, y, metric='f1', cv_folds=5):\n",
    "    \"\"\"Evaluate KNN performance using cross-validation\"\"\"\n",
    "    try:\n",
    "        # Parse individual based on workflow type\n",
    "        if len(individual) == X.shape[1]:  # Feature selection only\n",
    "            feature_mask = individual\n",
    "            k = 5  # Default k\n",
    "            weights = 'uniform'  # Default weights\n",
    "            p = 2  # Default p (Euclidean)\n",
    "        elif len(individual) == 3:  # Hyperparameters only\n",
    "            feature_mask = [1] * X.shape[1]  # Use all features\n",
    "            k = individual[0]\n",
    "            weights = 'distance' if individual[1] > 0.5 else 'uniform'\n",
    "            p = 1 if individual[2] < 0.33 else (2 if individual[2] < 0.66 else 3)\n",
    "        else:  # Joint optimization\n",
    "            feature_mask = individual[:-3]\n",
    "            k = individual[-3]\n",
    "            weights = 'distance' if individual[-2] > 0.5 else 'uniform'\n",
    "            p = 1 if individual[-1] < 0.33 else (2 if individual[-1] < 0.66 else 3)\n",
    "        \n",
    "        # Select features\n",
    "        selected_features = [i for i, mask in enumerate(feature_mask) if mask > 0.5]\n",
    "        if len(selected_features) == 0:\n",
    "            return (0.0,)  # No features selected\n",
    "        \n",
    "        X_selected = X.iloc[:, selected_features]\n",
    "        \n",
    "        # Ensure k is valid (adjust max k based on dataset size)\n",
    "        max_k = min(20, len(X_selected) // 5)  # Use at most 1/5 of data size for k\n",
    "        if isinstance(k, float):\n",
    "            k = max(1, min(int(k * max_k), max_k))\n",
    "        else:\n",
    "            k = max(1, min(k, max_k))\n",
    "        \n",
    "        # Create and evaluate model\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights=weights, p=p)\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        if metric == 'accuracy':\n",
    "            scores = cross_val_score(knn, X_selected, y, cv=cv, scoring='accuracy')\n",
    "        elif metric == 'precision':\n",
    "            scores = cross_val_score(knn, X_selected, y, cv=cv, scoring='precision')\n",
    "        elif metric == 'recall':\n",
    "            scores = cross_val_score(knn, X_selected, y, cv=cv, scoring='recall')\n",
    "        elif metric == 'f1':\n",
    "            scores = cross_val_score(knn, X_selected, y, cv=cv, scoring='f1')\n",
    "        elif metric == 'roc_auc':\n",
    "            scores = cross_val_score(knn, X_selected, y, cv=cv, scoring='roc_auc')\n",
    "        elif metric == 'pr_auc':\n",
    "            scores = cross_val_score(knn, X_selected, y, cv=cv, scoring='average_precision')\n",
    "        \n",
    "        return (np.mean(scores),)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return (0.0,)\n",
    "\n",
    "def test_final_model(individual, X_train, y_train, X_test, y_test, workflow_type):\n",
    "    \"\"\"Test the final model on test set and return all metrics\"\"\"\n",
    "    try:\n",
    "        # Parse individual based on workflow type\n",
    "        if workflow_type == 'features_only':\n",
    "            feature_mask = individual\n",
    "            k = 5\n",
    "            weights = 'uniform'\n",
    "            p = 2\n",
    "        elif workflow_type == 'params_only':\n",
    "            feature_mask = [1] * X_train.shape[1]\n",
    "            k = individual[0]\n",
    "            weights = 'distance' if individual[1] > 0.5 else 'uniform'\n",
    "            p = 1 if individual[2] < 0.33 else (2 if individual[2] < 0.66 else 3)\n",
    "        else:  # joint or sequential\n",
    "            feature_mask = individual[:-3]\n",
    "            k = individual[-3]\n",
    "            weights = 'distance' if individual[-2] > 0.5 else 'uniform'\n",
    "            p = 1 if individual[-1] < 0.33 else (2 if individual[-1] < 0.66 else 3)\n",
    "        \n",
    "        # Select features\n",
    "        selected_features = [i for i, mask in enumerate(feature_mask) if mask > 0.5]\n",
    "        if len(selected_features) == 0:\n",
    "            return {metric: 0.0 for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc']}\n",
    "        \n",
    "        X_train_selected = X_train.iloc[:, selected_features]\n",
    "        X_test_selected = X_test.iloc[:, selected_features]\n",
    "        \n",
    "        # Ensure k is valid\n",
    "        if isinstance(k, float):\n",
    "            k = max(1, min(int(k * 20), min(20, len(X_train_selected) - 1)))\n",
    "        else:\n",
    "            k = max(1, min(k, min(20, len(X_train_selected) - 1)))\n",
    "        \n",
    "        # Train and test model\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights=weights, p=p)\n",
    "        knn.fit(X_train_selected, y_train)\n",
    "        y_pred = knn.predict(X_test_selected)\n",
    "        y_pred_proba = knn.predict_proba(X_test_selected)[:, 1]\n",
    "        \n",
    "        # Calculate all metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'pr_auc': average_precision_score(y_test, y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {metric: 0.0 for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc']}\n",
    "\n",
    "# GA Workflow implementations\n",
    "def run_ga_workflow_a(X, y, metric='f1', n_runs=10, generations=20, pop_size=50):\n",
    "    \"\"\"Workflow A: Feature selection only\"\"\"\n",
    "    print(f\"Running Workflow A: Feature Selection Only ({metric})\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        print(f\"  Run {run + 1}/{n_runs}\")\n",
    "        \n",
    "        # Setup GA\n",
    "        toolbox = base.Toolbox()\n",
    "        toolbox.register(\"attr_bool\", random.random)\n",
    "        toolbox.register(\"individual\", tools.initRepeat, creator.Individual, \n",
    "                        toolbox.attr_bool, n=X.shape[1])\n",
    "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "        toolbox.register(\"evaluate\", evaluate_knn_cv, X=X, y=y, metric=metric)\n",
    "        toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "        toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n",
    "        toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "        \n",
    "        # Run GA\n",
    "        population = toolbox.population(n=pop_size)\n",
    "        hof = tools.HallOfFame(1)\n",
    "        \n",
    "        algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.2, \n",
    "                           ngen=generations, halloffame=hof, verbose=False)\n",
    "        \n",
    "        results.append(hof[0])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_ga_workflow_b(X, y, metric='f1', n_runs=10, generations=20, pop_size=50):\n",
    "    \"\"\"Workflow B: Hyperparameter tuning only\"\"\"\n",
    "    print(f\"Running Workflow B: Hyperparameter Tuning Only ({metric})\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        print(f\"  Run {run + 1}/{n_runs}\")\n",
    "        \n",
    "        # Setup GA for hyperparameters: [k, weights, p]\n",
    "        toolbox = base.Toolbox()\n",
    "        toolbox.register(\"attr_k\", random.randint, 1, 20)\n",
    "        toolbox.register(\"attr_weights\", random.random)\n",
    "        toolbox.register(\"attr_p\", random.random)\n",
    "        toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                        (toolbox.attr_k, toolbox.attr_weights, toolbox.attr_p))\n",
    "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "        toolbox.register(\"evaluate\", evaluate_knn_cv, X=X, y=y, metric=metric)\n",
    "        toolbox.register(\"mate\", tools.cxBlend, alpha=0.3)\n",
    "        toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.2, indpb=0.3)\n",
    "        toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "        \n",
    "        # Run GA\n",
    "        population = toolbox.population(n=pop_size)\n",
    "        hof = tools.HallOfFame(1)\n",
    "        \n",
    "        algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.2,\n",
    "                           ngen=generations, halloffame=hof, verbose=False)\n",
    "        \n",
    "        results.append(hof[0])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_ga_workflow_c(X, y, metric='f1', n_runs=10, generations=20, pop_size=50):\n",
    "    \"\"\"Workflow C: Joint feature selection + hyperparameter tuning\"\"\"\n",
    "    print(f\"Running Workflow C: Joint Optimization ({metric})\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        print(f\"  Run {run + 1}/{n_runs}\")\n",
    "        \n",
    "        # Setup GA for joint optimization: [features..., k, weights, p]\n",
    "        toolbox = base.Toolbox()\n",
    "        \n",
    "        def create_individual():\n",
    "            # Feature mask + hyperparameters\n",
    "            features = [random.random() for _ in range(X.shape[1])]\n",
    "            hyperparams = [random.randint(1, 20), random.random(), random.random()]\n",
    "            return creator.Individual(features + hyperparams)\n",
    "        \n",
    "        toolbox.register(\"individual\", create_individual)\n",
    "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "        toolbox.register(\"evaluate\", evaluate_knn_cv, X=X, y=y, metric=metric)\n",
    "        toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "        toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n",
    "        toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "        \n",
    "        # Run GA\n",
    "        population = toolbox.population(n=pop_size)\n",
    "        hof = tools.HallOfFame(1)\n",
    "        \n",
    "        algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.2,\n",
    "                           ngen=generations, halloffame=hof, verbose=False)\n",
    "        \n",
    "        results.append(hof[0])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_ga_workflow_d(X, y, metric='f1', n_runs=10, generations=20, pop_size=50):\n",
    "    \"\"\"Workflow D: Sequential hyperparameter tuning then feature selection\"\"\"\n",
    "    print(f\"Running Workflow D: Sequential Optimization ({metric})\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        print(f\"  Run {run + 1}/{n_runs}\")\n",
    "        \n",
    "        # Phase 1: Optimize hyperparameters\n",
    "        toolbox1 = base.Toolbox()\n",
    "        toolbox1.register(\"attr_k\", random.randint, 1, 20)\n",
    "        toolbox1.register(\"attr_weights\", random.random)\n",
    "        toolbox1.register(\"attr_p\", random.random)\n",
    "        toolbox1.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                         (toolbox1.attr_k, toolbox1.attr_weights, toolbox1.attr_p))\n",
    "        toolbox1.register(\"population\", tools.initRepeat, list, toolbox1.individual)\n",
    "        toolbox1.register(\"evaluate\", evaluate_knn_cv, X=X, y=y, metric=metric)\n",
    "        toolbox1.register(\"mate\", tools.cxBlend, alpha=0.3)\n",
    "        toolbox1.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.2, indpb=0.3)\n",
    "        toolbox1.register(\"select\", tools.selTournament, tournsize=3)\n",
    "        \n",
    "        population1 = toolbox1.population(n=pop_size)\n",
    "        hof1 = tools.HallOfFame(1)\n",
    "        \n",
    "        algorithms.eaSimple(population1, toolbox1, cxpb=0.7, mutpb=0.2,\n",
    "                           ngen=generations//2, halloffame=hof1, verbose=False)\n",
    "        \n",
    "        best_hyperparams = hof1[0]\n",
    "        \n",
    "        # Phase 2: Optimize features with fixed hyperparameters\n",
    "        def create_individual_phase2():\n",
    "            features = [random.random() for _ in range(X.shape[1])]\n",
    "            return creator.Individual(features + list(best_hyperparams))\n",
    "        \n",
    "        toolbox2 = base.Toolbox()\n",
    "        toolbox2.register(\"individual\", create_individual_phase2)\n",
    "        toolbox2.register(\"population\", tools.initRepeat, list, toolbox2.individual)\n",
    "        toolbox2.register(\"evaluate\", evaluate_knn_cv, X=X, y=y, metric=metric)\n",
    "        toolbox2.register(\"mate\", tools.cxTwoPoint)\n",
    "        toolbox2.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n",
    "        toolbox2.register(\"select\", tools.selTournament, tournsize=3)\n",
    "        \n",
    "        population2 = toolbox2.population(n=pop_size)\n",
    "        hof2 = tools.HallOfFame(1)\n",
    "        \n",
    "        algorithms.eaSimple(population2, toolbox2, cxpb=0.7, mutpb=0.2,\n",
    "                           ngen=generations//2, halloffame=hof2, verbose=False)\n",
    "        \n",
    "        results.append(hof2[0])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Main pipeline functions\n",
    "def main_pipeline():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    # Step 1: Upload and load datasets\n",
    "    print(\"=== Step 1: Upload Dataset Files ===\")\n",
    "    datasets = upload_and_load_files()\n",
    "    \n",
    "    if not datasets:\n",
    "        print(\"No datasets uploaded. Please restart and upload dataset files.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Select train and test versions\n",
    "    print(\"\\n=== Step 2: Select Train and Test Versions ===\")\n",
    "    print(\"Available datasets:\")\n",
    "    for i, name in enumerate(datasets.keys()):\n",
    "        print(f\"{i}: {name}\")\n",
    "    \n",
    "    train_idx = int(input(\"Select train dataset index: \"))\n",
    "    train_name = list(datasets.keys())[train_idx]\n",
    "    train_df = datasets[train_name]\n",
    "    \n",
    "    test_indices = input(\"Select test dataset indices (comma-separated): \")\n",
    "    test_indices = [int(x.strip()) for x in test_indices.split(',')]\n",
    "    test_names = [list(datasets.keys())[i] for i in test_indices]\n",
    "    test_dfs = [datasets[name] for name in test_names]\n",
    "    \n",
    "    print(f\"Train dataset: {train_name}\")\n",
    "    print(f\"Test datasets: {test_names}\")\n",
    "    \n",
    "    # Step 3: Preprocess datasets\n",
    "    print(\"\\n=== Step 3: Preprocessing ===\")\n",
    "    \n",
    "    # Preprocess train data\n",
    "    X_train, y_train, scaler, label_encoders, target_encoder = preprocess_dataset(train_df)\n",
    "    X_train_balanced, y_train_balanced = apply_smote(X_train, y_train)\n",
    "    \n",
    "    # Store feature names for hyperparameter display\n",
    "    feature_names = list(X_train.columns)\n",
    "    \n",
    "    # Preprocess test data\n",
    "    test_data = []\n",
    "    for test_df, test_name in zip(test_dfs, test_names):\n",
    "        X_test, y_test, _, _, _ = preprocess_dataset(test_df)\n",
    "        # Apply same scaling as train data\n",
    "        X_test_scaled = pd.DataFrame(scaler.transform(X_test), \n",
    "                                   columns=X_test.columns, index=X_test.index)\n",
    "        test_data.append((X_test_scaled, y_test, test_name))\n",
    "    \n",
    "    print(f\"Train data shape: {X_train_balanced.shape}\")\n",
    "    print(f\"Number of test datasets: {len(test_data)}\")\n",
    "    \n",
    "    # Step 4: Run GA workflows\n",
    "    print(\"\\n=== Step 4: Running GA Workflows ===\")\n",
    "    \n",
    "    metrics = ['precision', 'recall', 'f1', 'accuracy', 'roc_auc', 'pr_auc']\n",
    "    workflows = {\n",
    "        'A_features_only': run_ga_workflow_a,\n",
    "        'B_params_only': run_ga_workflow_b,\n",
    "        'C_joint': run_ga_workflow_c,\n",
    "        'D_sequential': run_ga_workflow_d\n",
    "    }\n",
    "    \n",
    "    all_results = {}\n",
    "    best_individuals = {}  # Store best individuals for hyperparameter display\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"\\n--- Optimizing for {metric.upper()} ---\")\n",
    "        \n",
    "        for workflow_name, workflow_func in workflows.items():\n",
    "            # Run GA optimization\n",
    "            ga_results = workflow_func(X_train_balanced, y_train_balanced, \n",
    "                                     metric=metric, n_runs=10, generations=20, pop_size=50)\n",
    "            \n",
    "            # Store best individuals for hyperparameter display\n",
    "            best_individuals[(metric, workflow_name)] = ga_results\n",
    "            \n",
    "            # Test each best model on all test sets\n",
    "            workflow_type = workflow_name.split('_', 1)[1]\n",
    "            \n",
    "            for run_idx, best_individual in enumerate(ga_results):\n",
    "                for X_test, y_test, test_name in test_data:\n",
    "                    test_metrics = test_final_model(best_individual, \n",
    "                                                  X_train_balanced, y_train_balanced,\n",
    "                                                  X_test, y_test, workflow_type)\n",
    "                    \n",
    "                    key = (metric, workflow_name, test_name, run_idx)\n",
    "                    all_results[key] = test_metrics\n",
    "    \n",
    "    # Display best hyperparameters\n",
    "    display_best_hyperparameters(best_individuals, list(workflows.keys()), metrics, feature_names)\n",
    "    \n",
    "    return all_results, test_names, metrics\n",
    "\n",
    "def aggregate_and_save_results(all_results, test_names, metrics):\n",
    "    \"\"\"Aggregate results and save to Excel\"\"\"\n",
    "    print(\"\\n=== Step 5: Aggregating Results ===\")\n",
    "    \n",
    "    workflows = ['A_features_only', 'B_params_only', 'C_joint', 'D_sequential']\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_data = []\n",
    "    \n",
    "    for opt_metric in metrics:\n",
    "        for workflow in workflows:\n",
    "            for test_name in test_names:\n",
    "                # Collect all runs for this combination\n",
    "                run_results = {metric: [] for metric in metrics}\n",
    "                \n",
    "                for run_idx in range(10):\n",
    "                    key = (opt_metric, workflow, test_name, run_idx)\n",
    "                    if key in all_results:\n",
    "                        for metric in metrics:\n",
    "                            run_results[metric].append(all_results[key][metric])\n",
    "                \n",
    "                # Calculate mean and std for each metric\n",
    "                row = {\n",
    "                    'Optimization_Metric': opt_metric,\n",
    "                    'Workflow': workflow,\n",
    "                    'Test_Dataset': test_name\n",
    "                }\n",
    "                \n",
    "                for metric in metrics:\n",
    "                    if run_results[metric]:\n",
    "                        mean_val = np.mean(run_results[metric])\n",
    "                        std_val = np.std(run_results[metric])\n",
    "                        row[f'{metric}_mean'] = mean_val\n",
    "                        row[f'{metric}_std'] = std_val\n",
    "                        row[f'{metric}_mean_std'] = f\"{mean_val:.4f}¬±{std_val:.4f}\"\n",
    "                    else:\n",
    "                        row[f'{metric}_mean'] = 0.0\n",
    "                        row[f'{metric}_std'] = 0.0\n",
    "                        row[f'{metric}_mean_std'] = \"0.0000¬±0.0000\"\n",
    "                \n",
    "                results_data.append(row)\n",
    "    \n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Save to Excel\n",
    "    results_df.to_excel('results.xlsx', index=False)\n",
    "    \n",
    "    print(f\"Results saved to results.xlsx\")\n",
    "    print(f\"Results shape: {results_df.shape}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\nSample results:\")\n",
    "    display_columns = ['Optimization_Metric', 'Workflow', 'Test_Dataset', 'f1_mean_std', 'accuracy_mean_std']\n",
    "    print(results_df[display_columns].head(10))\n",
    "    \n",
    "    # Download file\n",
    "    files.download('results.xlsx')\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def analyze_results():\n",
    "    \"\"\"Analyze and visualize the results\"\"\"\n",
    "    try:\n",
    "        results_df = pd.read_excel('results.xlsx')\n",
    "        \n",
    "        print(\"\\n=== RESULTS ANALYSIS ===\")\n",
    "        print(f\"Total combinations: {len(results_df)}\")\n",
    "        print(f\"Workflows: {results_df['Workflow'].unique()}\")\n",
    "        print(f\"Optimization metrics: {results_df['Optimization_Metric'].unique()}\")\n",
    "        print(f\"Test datasets: {results_df['Test_Dataset'].unique()}\")\n",
    "        \n",
    "        # Create summary plots\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: F1 scores by workflow and optimization metric\n",
    "        plt.subplot(2, 2, 1)\n",
    "        workflow_f1 = results_df.groupby(['Workflow', 'Optimization_Metric'])['f1_mean'].mean().unstack()\n",
    "        sns.heatmap(workflow_f1, annot=True, fmt='.3f', cmap='viridis')\n",
    "        plt.title('Average F1 Score by Workflow and Optimization Metric')\n",
    "        plt.ylabel('Workflow')\n",
    "        \n",
    "        # Plot 2: Accuracy scores by workflow and optimization metric\n",
    "        plt.subplot(2, 2, 2)\n",
    "        workflow_acc = results_df.groupby(['Workflow', 'Optimization_Metric'])['accuracy_mean'].mean().unstack()\n",
    "        sns.heatmap(workflow_acc, annot=True, fmt='.3f', cmap='viridis')\n",
    "        plt.title('Average Accuracy by Workflow and Optimization Metric')\n",
    "        plt.ylabel('Workflow')\n",
    "        \n",
    "        # Plot 3: ROC-AUC scores by workflow\n",
    "        plt.subplot(2, 2, 3)\n",
    "        workflow_roc = results_df.groupby('Workflow')['roc_auc_mean'].mean()\n",
    "        workflow_roc.plot(kind='bar')\n",
    "        plt.title('Average ROC-AUC by Workflow')\n",
    "        plt.ylabel('ROC-AUC')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Plot 4: PR-AUC scores by workflow\n",
    "        plt.subplot(2, 2, 4)\n",
    "        workflow_pr = results_df.groupby('Workflow')['pr_auc_mean'].mean()\n",
    "        workflow_pr.plot(kind='bar')\n",
    "        plt.title('Average PR-AUC by Workflow')\n",
    "        plt.ylabel('PR-AUC')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Best performing combinations\n",
    "        print(\"\\nTop 10 combinations by F1 score:\")\n",
    "        top_f1 = results_df.nlargest(10, 'f1_mean')[['Optimization_Metric', 'Workflow', 'Test_Dataset', 'f1_mean_std']]\n",
    "        print(top_f1.to_string(index=False))\n",
    "        \n",
    "        print(\"\\nTop 10 combinations by Accuracy:\")\n",
    "        top_acc = results_df.nlargest(10, 'accuracy_mean')[['Optimization_Metric', 'Workflow', 'Test_Dataset', 'accuracy_mean_std']]\n",
    "        print(top_acc.to_string(index=False))\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Results file not found. Please run the main pipeline first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing results: {str(e)}\")\n",
    "\n",
    "# Execute the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting KNN Genetic Algorithm Optimization Pipeline...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Run main pipeline\n",
    "        all_results, test_names, metrics = main_pipeline()\n",
    "        \n",
    "        # Aggregate and save results\n",
    "        final_results = aggregate_and_save_results(all_results, test_names, metrics)\n",
    "        \n",
    "        # Analyze results\n",
    "        analyze_results()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üéâ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"üìä Results have been saved to 'results.xlsx' and downloaded.\")\n",
    "        print(\"üìà Best hyperparameters displayed above.\")\n",
    "        print(\"üìã Analysis plots and top combinations shown.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\nüìã USAGE INSTRUCTIONS:\")\n",
    "print(\"1. Upload your Promise dataset files (CSV or ARFF format)\")\n",
    "print(\"2. Select train and test datasets when prompted\")\n",
    "print(\"3. Wait for the complete analysis (may take 30-60 minutes)\")\n",
    "print(\"4. Review hyperparameters, results, and analysis\")\n",
    "print(\"5. Download the generated Excel file with detailed results\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
