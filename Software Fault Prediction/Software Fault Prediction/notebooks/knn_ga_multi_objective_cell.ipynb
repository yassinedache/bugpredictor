{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Objective Genetic Algorithm for Feature Selection with KNN\n",
    "# Optimizing precision, recall, and F1-score simultaneously\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, accuracy_score, \n",
    "    roc_auc_score, average_precision_score, classification_report, \n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "import warnings\n",
    "import io\n",
    "from io import StringIO\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "from scipy.io import arff\n",
    "from google.colab import files\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Reset DEAP classes to avoid errors when re-running\n",
    "if 'FitnessMulti' in creator.__dict__:\n",
    "    del creator.FitnessMulti\n",
    "if 'MultiIndividual' in creator.__dict__:\n",
    "    del creator.MultiIndividual\n",
    "\n",
    "# Create DEAP multi-objective fitness and individual classes\n",
    "# Weights (1.0, 1.0, 1.0) means maximize all three objectives: precision, recall, f1\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(1.0, 1.0, 1.0))\n",
    "creator.create(\"MultiIndividual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "# --- Data Loading and Preprocessing Functions ---\n",
    "\n",
    "def upload_and_load_file():\n",
    "    \"\"\"Upload CSV or ARFF file and return dataframe\"\"\"\n",
    "    print(\"Please upload your Promise dataset file (CSV or ARFF format)...\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if not uploaded:\n",
    "        print(\"No file uploaded.\")\n",
    "        return None\n",
    "    \n",
    "    filename = list(uploaded.keys())[0]  # Get the first uploaded file\n",
    "    \n",
    "    if filename.endswith('.csv'):\n",
    "        df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
    "        print(f\"Loaded CSV {filename}: {df.shape}\")\n",
    "    elif filename.endswith('.arff'):\n",
    "        try:\n",
    "            # Try with default loading\n",
    "            data, meta = arff.loadarff(io.BytesIO(uploaded[filename]))\n",
    "            df = pd.DataFrame(data)\n",
    "        except TypeError:\n",
    "            # If TypeError about bytes-like object, decode to string first\n",
    "            arff_content = uploaded[filename].decode('utf-8')\n",
    "            data, meta = arff.loadarff(io.StringIO(arff_content))\n",
    "            df = pd.DataFrame(data)\n",
    "        \n",
    "        # Convert byte strings to regular strings for object columns\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                try:\n",
    "                    df[col] = df[col].str.decode('utf-8')\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "        \n",
    "        print(f\"Loaded ARFF {filename}: {df.shape}\")\n",
    "        print(f\"Attributes: {list(meta.names())}\")\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {filename}\")\n",
    "        return None\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_dataset(df, target_column=None):\n",
    "    \"\"\"Clean, encode, and prepare dataset for ML\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Impute missing values\n",
    "    if len(numeric_columns) > 0:\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "        df[numeric_columns] = num_imputer.fit_transform(df[numeric_columns])\n",
    "    \n",
    "    if len(categorical_columns) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df[categorical_columns] = cat_imputer.fit_transform(df[categorical_columns])\n",
    "    \n",
    "    # Handle target column\n",
    "    if target_column is None:\n",
    "        # Common target column names in Promise datasets\n",
    "        possible_targets = ['bug', 'defects', 'class', 'defective', 'Class']\n",
    "        target_column = None\n",
    "        \n",
    "        for col in possible_targets:\n",
    "            if col in df.columns:\n",
    "                target_column = col\n",
    "                break\n",
    "        \n",
    "        if target_column is None:\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "            target_column = input(\"Please specify the target column name: \")\n",
    "    \n",
    "    if target_column in df.columns:\n",
    "        y = df[target_column]\n",
    "        X = df.drop(columns=[target_column])\n",
    "        print(f\"Using '{target_column}' as target column\")\n",
    "    else:\n",
    "        # Assume last column is target\n",
    "        y = df.iloc[:, -1]\n",
    "        X = df.iloc[:, :-1]\n",
    "        print(f\"Using last column '{df.columns[-1]}' as target column\")\n",
    "    \n",
    "    # Encode categorical features\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # Encode target if categorical\n",
    "    if y.dtype == 'object' or y.dtype == 'bool':\n",
    "        target_encoder = LabelEncoder()\n",
    "        y = target_encoder.fit_transform(y)\n",
    "        print(f\"Target classes: {list(target_encoder.classes_)}\")\n",
    "        print(f\"Target encoding: {dict(zip(target_encoder.classes_, range(len(target_encoder.classes_))))}\")\n",
    "    else:\n",
    "        target_encoder = None\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    \n",
    "    return X_scaled, y, scaler, label_encoders, target_encoder\n",
    "\n",
    "def apply_smote(X, y, random_state=42):\n",
    "    \"\"\"Apply SMOTE to balance the dataset\"\"\"\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    \n",
    "    print(f\"Original class distribution: {np.bincount(y)}\")\n",
    "    print(f\"Balanced class distribution: {np.bincount(y_balanced)}\")\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "# --- Multi-Objective Genetic Algorithm Functions ---\n",
    "\n",
    "def evaluate_knn_multi_cv(individual, X, y, cv_folds=5):\n",
    "    \"\"\"Evaluate KNN performance using cross-validation for multiple metrics (precision, recall, f1)\"\"\"\n",
    "    try:\n",
    "        # For feature selection only\n",
    "        feature_mask = individual\n",
    "        k = 5  # Default k\n",
    "        weights = 'uniform'  # Default weights\n",
    "        p = 2  # Default p (Euclidean distance)\n",
    "        \n",
    "        # Select features\n",
    "        selected_features = [i for i, mask in enumerate(feature_mask) if mask > 0.5]\n",
    "        if len(selected_features) == 0:\n",
    "            return (0.0, 0.0, 0.0)  # No features selected\n",
    "        \n",
    "        X_selected = X.iloc[:, selected_features]\n",
    "        \n",
    "        # Ensure k is valid (adjust max k based on dataset size)\n",
    "        max_k = min(20, len(X_selected) // 5)  # Use at most 1/5 of data size for k\n",
    "        k = max(1, min(k, max_k))\n",
    "        \n",
    "        # Create model\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights=weights, p=p)\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Calculate all three metrics simultaneously\n",
    "        precision_scores = cross_val_score(knn, X_selected, y, cv=cv, scoring='precision')\n",
    "        recall_scores = cross_val_score(knn, X_selected, y, cv=cv, scoring='recall')\n",
    "        f1_scores = cross_val_score(knn, X_selected, y, cv=cv, scoring='f1')\n",
    "        \n",
    "        return (np.mean(precision_scores), np.mean(recall_scores), np.mean(f1_scores))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation error: {str(e)}\")\n",
    "        return (0.0, 0.0, 0.0)\n",
    "\n",
    "def run_multi_objective_ga(X, y, n_runs=10, generations=20, pop_size=50):\n",
    "    \"\"\"Run multi-objective genetic algorithm for feature selection\"\"\"\n",
    "    print(f\"Running Multi-Objective Feature Selection (precision, recall, f1)\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        print(f\"  Run {run + 1}/{n_runs}\")\n",
    "        \n",
    "        # Setup GA\n",
    "        toolbox = base.Toolbox()\n",
    "        toolbox.register(\"attr_bool\", random.random)\n",
    "        toolbox.register(\"individual\", tools.initRepeat, creator.MultiIndividual, \n",
    "                         toolbox.attr_bool, n=X.shape[1])\n",
    "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "        toolbox.register(\"evaluate\", evaluate_knn_multi_cv, X=X, y=y)\n",
    "        toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "        toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n",
    "        toolbox.register(\"select\", tools.selNSGA2)\n",
    "        \n",
    "        # Run GA\n",
    "        population = toolbox.population(n=pop_size)\n",
    "        hof = tools.ParetoFront()  # Store non-dominated solutions\n",
    "        \n",
    "        # Use NSGA-II algorithm for multi-objective optimization\n",
    "        algorithms.eaMuPlusLambda(\n",
    "            population, toolbox, \n",
    "            mu=pop_size,          # Number of individuals to select for next generation\n",
    "            lambda_=pop_size,     # Number of children to produce\n",
    "            cxpb=0.7,             # Crossover probability\n",
    "            mutpb=0.2,            # Mutation probability\n",
    "            ngen=generations,     # Number of generations\n",
    "            halloffame=hof,       # Hall of Fame with Pareto-optimal solutions\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Store all non-dominated solutions from this run\n",
    "        for ind in hof:\n",
    "            results.append(ind)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_solution_with_cv(individual, X, y, cv_folds=5):\n",
    "    \"\"\"Evaluate a solution using cross-validation and return detailed metrics\"\"\"\n",
    "    try:\n",
    "        # Parse individual for feature selection\n",
    "        feature_mask = individual\n",
    "        k = 5  # Default k\n",
    "        weights = 'uniform'  # Default weights\n",
    "        p = 2  # Default p (Euclidean)\n",
    "        \n",
    "        # Select features\n",
    "        selected_features = [i for i, mask in enumerate(feature_mask) if mask > 0.5]\n",
    "        if len(selected_features) == 0:\n",
    "            return {metric: 0.0 for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']}\n",
    "        \n",
    "        X_selected = X.iloc[:, selected_features]\n",
    "        \n",
    "        # Create model\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights=weights, p=p)\n",
    "        \n",
    "        # Define CV strategy\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Perform cross-validation with multiple metrics\n",
    "        scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "        cv_results = cross_validate(knn, X_selected, y, cv=cv, scoring=scoring)\n",
    "        \n",
    "        # Aggregate results\n",
    "        metrics = {}\n",
    "        for metric in scoring:\n",
    "            metrics[metric] = np.mean(cv_results[f'test_{metric}'])\n",
    "            metrics[f'{metric}_std'] = np.std(cv_results[f'test_{metric}'])\n",
    "        \n",
    "        metrics['selected_features'] = len(selected_features)\n",
    "        metrics['feature_indices'] = selected_features\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation error: {str(e)}\")\n",
    "        return {metric: 0.0 for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'selected_features']}\n",
    "\n",
    "def select_best_solution(solutions, evaluation_results, metric='f1'):\n",
    "    \"\"\"Select the best solution based on a specific metric\"\"\"\n",
    "    if not solutions or not evaluation_results:\n",
    "        return None, None\n",
    "        \n",
    "    # Find best solution based on the provided metric\n",
    "    best_idx = np.argmax([result[metric] for result in evaluation_results])\n",
    "    best_solution = solutions[best_idx]\n",
    "    best_result = evaluation_results[best_idx]\n",
    "    \n",
    "    return best_solution, best_result\n",
    "\n",
    "def build_and_save_best_model(best_solution, X, y, feature_names, save_path='best_knn_model.pkl'):\n",
    "    \"\"\"Build and save the best KNN model based on the selected solution\"\"\"\n",
    "    # Parse individual for feature selection\n",
    "    feature_mask = best_solution\n",
    "    k = 5  # Default k\n",
    "    weights = 'uniform'  # Default weights\n",
    "    p = 2  # Default p (Euclidean)\n",
    "    \n",
    "    # Select features\n",
    "    selected_features = [i for i, mask in enumerate(feature_mask) if mask > 0.5]\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    \n",
    "    # Build the model\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights=weights, p=p)\n",
    "    knn.fit(X_selected, y)\n",
    "    \n",
    "    # Create a dictionary with the model and metadata\n",
    "    model_info = {\n",
    "        'model': knn,\n",
    "        'selected_feature_indices': selected_features,\n",
    "        'selected_feature_names': [feature_names[i] for i in selected_features],\n",
    "        'hyperparameters': {\n",
    "            'n_neighbors': k,\n",
    "            'weights': weights,\n",
    "            'p': p\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(model_info, save_path)\n",
    "    print(f\"\\nBest model saved to: {save_path}\")\n",
    "    files.download(save_path)\n",
    "    \n",
    "    return model_info\n",
    "\n",
    "# --- Results Analysis Functions ---\n",
    "\n",
    "def display_feature_selection_results(solutions, feature_names):\n",
    "    \"\"\"Display feature selection statistics for Pareto-optimal solutions\"\"\"\n",
    "    print(\"\\n=== Feature Selection Analysis ===\")\n",
    "    \n",
    "    if not solutions:\n",
    "        print(\"No solutions to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Count features\n",
    "    feature_counts = defaultdict(int)\n",
    "    selected_feature_counts = []\n",
    "    \n",
    "    for ind in solutions:\n",
    "        selected_features = [i for i, mask in enumerate(ind) if mask > 0.5]\n",
    "        selected_feature_counts.append(len(selected_features))\n",
    "        for i in selected_features:\n",
    "            if i < len(feature_names):\n",
    "                feature_counts[feature_names[i]] += 1\n",
    "    \n",
    "    # Feature count statistics\n",
    "    print(f\"Number of Pareto-optimal solutions: {len(solutions)}\")\n",
    "    print(f\"Average number of features: {np.mean(selected_feature_counts):.2f}\")\n",
    "    print(f\"Range of features: {min(selected_feature_counts)} - {max(selected_feature_counts)}\")\n",
    "    \n",
    "    # Most frequently selected features\n",
    "    top_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\nTop 10 most frequently selected features:\")\n",
    "    for feat, count in top_features[:10]:\n",
    "        percentage = (count / len(solutions)) * 100\n",
    "        print(f\"  • {feat}: {count}/{len(solutions)} solutions ({percentage:.1f}%)\")\n",
    "\n",
    "def plot_pareto_front(solutions, evaluation_results):\n",
    "    \"\"\"Plot Pareto front and related visualizations\"\"\"\n",
    "    if not solutions or not evaluation_results:\n",
    "        print(\"Not enough data for visualization\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # 1. Precision vs Recall scatter plot (with feature count as size)\n",
    "    plt.subplot(2, 2, 1)\n",
    "    precisions = [ind.fitness.values[0] for ind in solutions]\n",
    "    recalls = [ind.fitness.values[1] for ind in solutions]\n",
    "    f1s = [ind.fitness.values[2] for ind in solutions]\n",
    "    feature_counts = [sum(1 for mask in ind if mask > 0.5) for ind in solutions]\n",
    "    \n",
    "    scatter = plt.scatter(precisions, recalls, c=f1s, s=[fc*10 for fc in feature_counts], \n",
    "                        alpha=0.7, cmap='viridis')\n",
    "    plt.colorbar(scatter, label='F1 Score')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title('Pareto Front: Precision vs Recall')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 2. Feature count vs F1 score\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(feature_counts, f1s, c=precisions, cmap='plasma', alpha=0.7)\n",
    "    plt.colorbar(label='Precision')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Feature Count vs F1 Score')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 3. Bar chart of top features\n",
    "    plt.subplot(2, 2, 3)\n",
    "    feature_counts_dict = defaultdict(int)\n",
    "    for ind in solutions:\n",
    "        for i, mask in enumerate(ind):\n",
    "            if mask > 0.5 and i < len(evaluation_results[0]['feature_names']):\n",
    "                feature_counts_dict[evaluation_results[0]['feature_names'][i]] += 1\n",
    "    \n",
    "    top_n = 10\n",
    "    top_features = sorted(feature_counts_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    feature_labels = [f\"{name[:15]}...\" if len(name) > 15 else name for name, _ in top_features]\n",
    "    feature_values = [count for _, count in top_features]\n",
    "    \n",
    "    bars = plt.bar(range(len(top_features)), feature_values, color='skyblue')\n",
    "    plt.xticks(range(len(top_features)), feature_labels, rotation=45, ha='right')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Selection Frequency')\n",
    "    plt.title(f'Top {top_n} Most Selected Features')\n",
    "    \n",
    "    # 4. Performance metrics comparison\n",
    "    plt.subplot(2, 2, 4)\n",
    "    metrics = ['precision', 'recall', 'f1', 'accuracy']\n",
    "    avg_metrics = {metric: np.mean([result[metric] for result in evaluation_results]) for metric in metrics}\n",
    "    \n",
    "    bars = plt.bar(metrics, [avg_metrics[m] for m in metrics], color=['blue', 'green', 'red', 'purple'])\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.title('Average Performance Metrics (Cross-Validation)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_best_model_details(best_solution, best_result, feature_names):\n",
    "    \"\"\"Display detailed information about the best model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST MODEL DETAILS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Feature selection details\n",
    "    selected_features = [i for i, mask in enumerate(best_solution) if mask > 0.5]\n",
    "    selected_feature_names = [feature_names[i] for i in selected_features if i < len(feature_names)]\n",
    "    \n",
    "    # Performance metrics\n",
    "    metrics = {\n",
    "        'Precision': best_result['precision'],\n",
    "        'Recall': best_result['recall'],\n",
    "        'F1-Score': best_result['f1'],\n",
    "        'Accuracy': best_result['accuracy'],\n",
    "        'ROC-AUC': best_result['roc_auc']\n",
    "    }\n",
    "    \n",
    "    metrics_std = {\n",
    "        'Precision': best_result['precision_std'],\n",
    "        'Recall': best_result['recall_std'],\n",
    "        'F1-Score': best_result['f1_std'],\n",
    "        'Accuracy': best_result['accuracy_std'],\n",
    "        'ROC-AUC': best_result['roc_auc_std']\n",
    "    }\n",
    "    \n",
    "    print(\"\\n1. PERFORMANCE METRICS (5-fold Cross-Validation)\")\n",
    "    print(\"-\"*60)\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  • {metric}: {value:.4f} (±{metrics_std[metric]:.4f})\")\n",
    "    \n",
    "    print(\"\\n2. SELECTED FEATURES\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"  • Number of selected features: {len(selected_feature_names)} out of {len(feature_names)}\")\n",
    "    print(f\"  • Selected features:\")\n",
    "    for i, feature in enumerate(selected_feature_names):\n",
    "        print(f\"    {i+1}. {feature}\")\n",
    "    \n",
    "    print(\"\\n3. MODEL HYPERPARAMETERS\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"  • KNN Algorithm: K-Nearest Neighbors\")\n",
    "    print(f\"  • n_neighbors (k): 5\")\n",
    "    print(f\"  • weights: uniform\")\n",
    "    print(f\"  • metric: euclidean (p=2)\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "def save_results_to_excel(pareto_solutions, evaluation_results, feature_names, best_idx=None):\n",
    "    \"\"\"Save results to Excel file with best solution highlighted\"\"\"\n",
    "    # Create solution details dataframe\n",
    "    solution_data = []\n",
    "    \n",
    "    for i, ind in enumerate(pareto_solutions):\n",
    "        fitness = ind.fitness.values\n",
    "        selected_features = [j for j, mask in enumerate(ind) if mask > 0.5]\n",
    "        selected_feature_names = [feature_names[j] for j in selected_features if j < len(feature_names)]\n",
    "        \n",
    "        # Get evaluation results for this solution\n",
    "        eval_result = evaluation_results[i]\n",
    "        \n",
    "        solution_data.append({\n",
    "            'Solution': i+1,\n",
    "            'Is_Best': 'Yes' if i == best_idx else 'No',\n",
    "            'Precision': fitness[0],\n",
    "            'Recall': fitness[1],\n",
    "            'F1': fitness[2],\n",
    "            'Feature_Count': len(selected_features),\n",
    "            'CV_Precision': eval_result['precision'],\n",
    "            'CV_Precision_STD': eval_result['precision_std'],\n",
    "            'CV_Recall': eval_result['recall'],\n",
    "            'CV_Recall_STD': eval_result['recall_std'],\n",
    "            'CV_F1': eval_result['f1'],\n",
    "            'CV_F1_STD': eval_result['f1_std'],\n",
    "            'CV_Accuracy': eval_result['accuracy'],\n",
    "            'CV_ROC_AUC': eval_result['roc_auc'],\n",
    "            'Selected_Features': ', '.join(selected_feature_names)\n",
    "        })\n",
    "    \n",
    "    solutions_df = pd.DataFrame(solution_data)\n",
    "    \n",
    "    # Create Excel file\n",
    "    with pd.ExcelWriter('multi_objective_feature_selection_results.xlsx') as writer:\n",
    "        solutions_df.to_excel(writer, sheet_name='Pareto_Solutions', index=False)\n",
    "        \n",
    "        # If there's a best solution, create a separate sheet for it\n",
    "        if best_idx is not None:\n",
    "            best_solution = solutions_df[solutions_df['Is_Best'] == 'Yes']\n",
    "            best_solution.to_excel(writer, sheet_name='Best_Solution', index=False)\n",
    "    \n",
    "    print(\"Results saved to 'multi_objective_feature_selection_results.xlsx'\")\n",
    "    files.download('multi_objective_feature_selection_results.xlsx')\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(\"=== Multi-Objective Feature Selection for KNN with GA ===\")\n",
    "    print(\"Simultaneously optimizing: Precision, Recall, and F1-score\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Step 1: Upload and load dataset\n",
    "    print(\"\\n=== Step 1: Upload Dataset File ===\")\n",
    "    df = upload_and_load_file()\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"No dataset uploaded. Please restart and upload a dataset file.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Step 2: Preprocess dataset\n",
    "    print(\"\\n=== Step 2: Preprocessing ===\")\n",
    "    X, y, scaler, label_encoders, target_encoder = preprocess_dataset(df)\n",
    "    \n",
    "    # Apply SMOTE if there's class imbalance\n",
    "    class_counts = np.bincount(y)\n",
    "    if max(class_counts) / min(class_counts) > 1.5:  # If imbalance ratio is greater than 1.5\n",
    "        print(\"Class imbalance detected, applying SMOTE...\")\n",
    "        X, y = apply_smote(X, y)\n",
    "    else:\n",
    "        print(\"Class distribution is relatively balanced, no SMOTE needed.\")\n",
    "        print(f\"Class distribution: {class_counts}\")\n",
    "    \n",
    "    # Store feature names for analysis\n",
    "    feature_names = list(X.columns)\n",
    "    \n",
    "    print(f\"Features: {len(feature_names)}\")\n",
    "    print(f\"First 5 features: {feature_names[:5]}\")\n",
    "    \n",
    "    # Step 3: Run Multi-Objective GA\n",
    "    print(\"\\n=== Step 3: Running Multi-Objective Feature Selection ===\")\n",
    "    pareto_solutions = run_multi_objective_ga(\n",
    "        X, y, \n",
    "        n_runs=5,          # Number of independent GA runs\n",
    "        generations=20,    # Number of generations per run\n",
    "        pop_size=50        # Population size per generation\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFound {len(pareto_solutions)} Pareto-optimal solutions\")\n",
    "    \n",
    "    # Print Pareto-optimal solutions\n",
    "    print(\"\\n=== Pareto-Optimal Solutions ===\")\n",
    "    for i, ind in enumerate(pareto_solutions):\n",
    "        fitness = ind.fitness.values\n",
    "        selected_features = sum(1 for mask in ind if mask > 0.5)\n",
    "        print(f\"Solution {i+1}: Precision={fitness[0]:.4f}, Recall={fitness[1]:.4f}, F1={fitness[2]:.4f}, Features={selected_features}\")\n",
    "    \n",
    "    # Step 4: Evaluate solutions using cross-validation\n",
    "    print(\"\\n=== Step 4: Evaluating Solutions with Cross-Validation ===\")\n",
    "    \n",
    "    evaluation_results = []\n",
    "    \n",
    "    for i, solution in enumerate(pareto_solutions):\n",
    "        print(f\"Evaluating solution {i+1}/{len(pareto_solutions)}\")\n",
    "        \n",
    "        # Evaluate with 5-fold cross-validation\n",
    "        metrics = evaluate_solution_with_cv(solution, X, y, cv_folds=5)\n",
    "        \n",
    "        # Add additional info\n",
    "        metrics['solution_id'] = i+1\n",
    "        metrics['feature_names'] = feature_names\n",
    "        \n",
    "        evaluation_results.append(metrics)\n",
    "        \n",
    "        print(f\"  Precision: {metrics['precision']:.4f} (±{metrics['precision_std']:.4f})\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f} (±{metrics['recall_std']:.4f})\")\n",
    "        print(f\"  F1-Score: {metrics['f1']:.4f} (±{metrics['f1_std']:.4f})\")\n",
    "        print(f\"  Selected Features: {metrics['selected_features']}\")\n",
    "    \n",
    "    # Step 5: Select best solution (using F1 score by default)\n",
    "    print(\"\\n=== Step 5: Selecting Best Solution ===\")\n",
    "    best_solution, best_result = select_best_solution(pareto_solutions, evaluation_results, metric='f1')\n",
    "    best_idx = evaluation_results.index(best_result)\n",
    "    \n",
    "    # Step 6: Build and save the best model\n",
    "    print(\"\\n=== Step 6: Building and Saving Best Model ===\")\n",
    "    model_info = build_and_save_best_model(best_solution, X, y, feature_names, 'best_knn_multi_objective_model.pkl')\n",
    "    \n",
    "    # Step 7: Display detailed results for the best model\n",
    "    display_best_model_details(best_solution, best_result, feature_names)\n",
    "    \n",
    "    # Step 8: Analyze and Visualize Results\n",
    "    print(\"\\n=== Step 7: Analyzing Results ===\")\n",
    "    \n",
    "    # Display feature selection statistics\n",
    "    display_feature_selection_results(pareto_solutions, feature_names)\n",
    "    \n",
    "    # Plot Pareto front and other visualizations\n",
    "    plot_pareto_front(pareto_solutions, evaluation_results)\n",
    "    \n",
    "    # Save results to Excel with best solution highlighted\n",
    "    save_results_to_excel(pareto_solutions, evaluation_results, feature_names, best_idx=best_idx)\n",
    "    \n",
    "    print(\"\\n=== Pipeline Completed Successfully ===\")\n",
    "    print(\"Multi-objective feature selection has provided a set of Pareto-optimal solutions\")\n",
    "    print(\"Each solution represents a different trade-off between precision, recall, and F1-score\")\n",
    "    print(\"The best model has been saved and results have been generated\")\n",
    "\n",
    "# Execute the main pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
